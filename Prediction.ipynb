{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicition\n",
    "This notebook details the pipeline for next-chord prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data \n",
    "When loading the chord dataset, we can choose whether to keep sections in major or minor key, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_train_test_sentences, all_composers, non_noisy_composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Choose which composers to train on and which to test on\n",
    "composers = all_composers\n",
    "#composers = non_noisy_composers\n",
    "test_composers = ['Pleyel']\n",
    "\n",
    "train_sentences, test_sentences = load_train_test_sentences(composers, test_composers, key_mode='MAJOR')\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Word2Vec\n",
    "Several hyperparameters to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from load_data import get_chord_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore words with a lower frequency frequency than this\n",
    "min_count = 5\n",
    "# Size of the embedding space\n",
    "size = 20\n",
    "# Neighborhood of the focus word to study\n",
    "window = 2\n",
    "# 0 for CBOW, 1 for skip-gram\n",
    "sg = 1\n",
    "# Number of iterations (epochs)\n",
    "iter = 500\n",
    "\n",
    "# The first argument has to be a list of lists of words\n",
    "w2v_model = Word2Vec(train_sentences, min_count=min_count, size=size, window=window, sg=sg, iter=iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['I:MAJ', 'V:MAJ', 'IV:MAJ', '#IV:DIM', 'II:MAJ', 'VI:MIN', 'bVII:MAJ', 'VII:DIM', 'III:MAJ', 'VI:MAJ', 'II:MIN', '#I:DIM', 'V:MIN', 'III:DIM', 'II:DIM', 'IV:MIN', 'I:DIM', '#V:DIM', 'VII:MAJ', 'III:MIN', 'I:MIN', 'VII:MIN', 'bIII:MAJ', 'bVI:MAJ', '#II:DIM', 'VI:DIM', 'bbVII:MAJ', 'I:AUG', 'IV:AUG', 'V:AUG', '#VI:DIM', 'bVII:MIN', '#II:MAJ', 'II:AUG', '#V:MAJ', 'bVI:AUG', 'bIII:AUG', 'bII:MAJ', '#IV:MIN', 'bVII:AUG', '#IV:MAJ', 'bV:MAJ', 'bIV:MAJ', 'bVII:DIM', 'V:DIM', 'bI:MAJ', 'IV:DIM', 'bIII:MIN', '#I:MAJ', 'bI:MIN', 'bVI:MIN', '#VI:MAJ', 'bIII:DIM', '#VII:DIM', '#I:MIN', '#II:MIN', 'III:AUG', '#III:MAJ', 'bII:MIN', '#V:MIN', '##IV:DIM', '#III:DIM'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Train the LSTM predictor on the same dataset as the Word2Vec model, then test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTMPredictor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 5000 : average loss = 2.481371134388447\n",
      "Iteration 10000 : average loss = 2.696560618257523\n",
      "Iteration 15000 : average loss = 2.3028311231821776\n",
      "Iteration 20000 : average loss = 2.0471320151239634\n",
      "Iteration 25000 : average loss = 2.1376013145715\n",
      "Iteration 30000 : average loss = 1.7969899514466525\n",
      "Iteration 35000 : average loss = 1.3552037417069078\n",
      "Iteration 40000 : average loss = 1.5395047161892057\n",
      "Iteration 45000 : average loss = 1.8766031429111958\n",
      "Iteration 50000 : average loss = 2.259413107815385\n",
      "Iteration 55000 : average loss = 1.6999046997725964\n",
      "Iteration 60000 : average loss = 1.4278137263149022\n",
      "Iteration 65000 : average loss = 1.9280818780869247\n",
      "Closing epoch 0 \n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 5000 : average loss = 2.0709528877168895\n",
      "Iteration 10000 : average loss = 2.586650201436877\n",
      "Iteration 15000 : average loss = 2.1332288523107765\n",
      "Iteration 20000 : average loss = 1.9335909777104854\n",
      "Iteration 25000 : average loss = 2.0599059022992847\n",
      "Iteration 30000 : average loss = 1.7510607688143849\n",
      "Iteration 35000 : average loss = 1.3260333259269594\n",
      "Iteration 40000 : average loss = 1.5079742130428553\n",
      "Iteration 45000 : average loss = 1.8588117510259152\n",
      "Iteration 50000 : average loss = 2.217305918478966\n",
      "Iteration 55000 : average loss = 1.6922787253364922\n",
      "Iteration 60000 : average loss = 1.42055624255836\n",
      "Iteration 65000 : average loss = 1.9441925573349\n",
      "Closing epoch 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_predictor = LSTMPredictor(w2v_model, 15)\n",
    "optimiser = optim.Adam(lstm_predictor.parameters(), lr=0.001)\n",
    "\n",
    "lstm_predictor.learn(train_sentences, optimiser, 2)\n",
    "# Training takes a couple minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.5343383584589615\n",
      "Accuracy by chord\n",
      " {'V:MAJ': 0.33507853403141363, 'I:MAJ': 0.875, 'IV:MAJ': 0.896551724137931, 'II:MIN': 0.1794871794871795, 'VI:MIN': 0.2916666666666667, 'VII:DIM': 0.0, 'III:MIN': 0.0, 'I:MIN': 0.0, 'VI:MAJ': 0.0, '#IV:DIM': 0.0, 'II:MAJ': 0.0, 'IV:MIN': 0.0, 'III:MAJ': 0.0, '#II:DIM': 0.0, '#I:DIM': 0.0, 'I:AUG': 0.0, '#V:DIM': 0.0, 'VI:DIM': 0.0, 'III:DIM': 0.0, 'II:DIM': 0.0}\n",
      "Occurrences by chord\n",
      " {'V:MAJ': 191, 'I:MAJ': 216, 'IV:MAJ': 58, 'II:MIN': 39, 'VI:MIN': 24, 'VII:DIM': 20, 'III:MIN': 2, 'I:MIN': 4, 'VI:MAJ': 7, '#IV:DIM': 6, 'II:MAJ': 5, 'IV:MIN': 1, 'III:MAJ': 4, '#II:DIM': 3, '#I:DIM': 8, 'I:AUG': 2, '#V:DIM': 4, 'VI:DIM': 1, 'III:DIM': 1, 'II:DIM': 1}\n"
     ]
    }
   ],
   "source": [
    "accuracy_total, accuracy_by_chord, occurrences_by_chord = lstm_predictor.test(test_sentences)\n",
    "\n",
    "print('Total accuracy:', accuracy_total)\n",
    "print('Accuracy by chord\\n', accuracy_by_chord)\n",
    "print('Occurrences by chord\\n', occurrences_by_chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
